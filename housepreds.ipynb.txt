{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"},{"sourceId":185398,"sourceType":"modelInstanceVersion","modelInstanceId":158055,"modelId":180454}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:43.178604Z","iopub.execute_input":"2024-12-02T07:52:43.179084Z","iopub.status.idle":"2024-12-02T07:52:43.638089Z","shell.execute_reply.started":"2024-12-02T07:52:43.179045Z","shell.execute_reply":"2024-12-02T07:52:43.637042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\nfrom xgboost import XGBRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:43.640174Z","iopub.execute_input":"2024-12-02T07:52:43.640781Z","iopub.status.idle":"2024-12-02T07:52:44.953941Z","shell.execute_reply.started":"2024-12-02T07:52:43.640731Z","shell.execute_reply":"2024-12-02T07:52:44.952573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#read Data\ndf_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\nprint(f\" train head is:\", df_train.head())\nprint(f\" test head is:\", df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:44.955303Z","iopub.execute_input":"2024-12-02T07:52:44.955774Z","iopub.status.idle":"2024-12-02T07:52:45.053793Z","shell.execute_reply.started":"2024-12-02T07:52:44.955706Z","shell.execute_reply":"2024-12-02T07:52:45.052586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_train.info())\nprint(df_test.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:45.055963Z","iopub.execute_input":"2024-12-02T07:52:45.056282Z","iopub.status.idle":"2024-12-02T07:52:45.097957Z","shell.execute_reply.started":"2024-12-02T07:52:45.056251Z","shell.execute_reply":"2024-12-02T07:52:45.096886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#ploting distributuin\ndf_train.drop('Id', axis=1, inplace=True)\ntest_id = df_test['Id']\ndf_test.drop('Id', axis=1, inplace=True)\nplt.figure(figsize=(10,6))\nplt.hist(df_train['SalePrice'], bins=30, color ='blue', edgecolor='black')\nplt.title('Distribution of house price')\nplt.xlabel('Price')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:45.099203Z","iopub.execute_input":"2024-12-02T07:52:45.099526Z","iopub.status.idle":"2024-12-02T07:52:45.43209Z","shell.execute_reply.started":"2024-12-02T07:52:45.099496Z","shell.execute_reply":"2024-12-02T07:52:45.430979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nsns.boxplot(df_train['SalePrice'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:53:25.027152Z","iopub.execute_input":"2024-12-02T07:53:25.027694Z","iopub.status.idle":"2024-12-02T07:53:25.249924Z","shell.execute_reply.started":"2024-12-02T07:53:25.027619Z","shell.execute_reply":"2024-12-02T07:53:25.248777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Correlation between numerical features and target\nnumeric_features=df_train.select_dtypes(include=['int64','float64'])\ncorrelation_matrix = numeric_features.corr()\ncorrelation_with_target = correlation_matrix['SalePrice'].sort_values(ascending=False)\nprint(correlation_with_target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:53:33.547088Z","iopub.execute_input":"2024-12-02T07:53:33.547479Z","iopub.status.idle":"2024-12-02T07:53:33.565417Z","shell.execute_reply.started":"2024-12-02T07:53:33.547444Z","shell.execute_reply":"2024-12-02T07:53:33.564347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#visualize correlation\nplt.figure(figsize=(10,8))\ncorrelation_with_target.drop('SalePrice').plot(kind='bar', color='red')\nplt.title('Correlation with SalePrice')\nplt.xlabel('Features')\nplt.ylabel('Correlation')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:53:37.616464Z","iopub.execute_input":"2024-12-02T07:53:37.616845Z","iopub.status.idle":"2024-12-02T07:53:38.174534Z","shell.execute_reply.started":"2024-12-02T07:53:37.616814Z","shell.execute_reply":"2024-12-02T07:53:38.173343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#statistical parameters\nfrom IPython.display import display\n\n\nmean_price = df_train['SalePrice'].mean()\nmedian_price = df_train['SalePrice'].median()\nmode_price = df_train['SalePrice'].mode()[0]\nstd_price = df_train['SalePrice'].std()\nvar_price = df_train['SalePrice'].var()\nmin_price = df_train['SalePrice'].min()\nmax_price = df_train['SalePrice'].max()\nrange_price = max_price - min_price\nq1 = df_train['SalePrice'].quantile(0.25)\nq3 = df_train['SalePrice'].quantile(0.75)\niqr = q3 - q1\nskewness = df_train['SalePrice'].skew()\nkurtosis = df_train['SalePrice'].kurt()\n\nstats = ({\n     'statistics':[\n         'Mean', 'Median', 'Mode', 'Standard Deviation','Variance',\n                   'Minimum','Maximum', 'Range','Q1 (25th percentile)', 'Q3 (75th percentile)',\n                   'IQR','Skewness','Kurtosis'\n                   ],\n     'Value':[\n         mean_price,median_price,mode_price,std_price,var_price,\n         min_price,max_price,range_price, q1, q3, iqr , skewness, kurtosis\n         ]\n})\nstats_table = pd.DataFrame(stats)\ndisplay(stats_table)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:53:42.010014Z","iopub.execute_input":"2024-12-02T07:53:42.0104Z","iopub.status.idle":"2024-12-02T07:53:42.033023Z","shell.execute_reply.started":"2024-12-02T07:53:42.010366Z","shell.execute_reply":"2024-12-02T07:53:42.031935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Median(1.630000e+05\n) is less than Mean (1.809212e+05) and skewness is equal to 1.8 which indicates positive skewness.\nAlso Range is 7.201000e+05 which is large.\nin addition, kurtosis is equal to 6, which is called lepokurtic.\nin conclusion, these three parameters indicate the existence of outliers. so in the following we detect outliers to manage them.\n","metadata":{}},{"cell_type":"code","source":"#calculating outliers\nlower_bound = q1-1.5*iqr\nupper_bound = q3+1.5*iqr\nprint(f\"lower bound :{lower_bound}\")\nprint(f\"upper bound :{upper_bound}\")\noutliers = df_train[(df_train['SalePrice']<lower_bound) | (df_train['SalePrice']>upper_bound)]\nprint(f\"number of outliers is:{len(outliers)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:53:47.043787Z","iopub.execute_input":"2024-12-02T07:53:47.044744Z","iopub.status.idle":"2024-12-02T07:53:47.052452Z","shell.execute_reply.started":"2024-12-02T07:53:47.044669Z","shell.execute_reply":"2024-12-02T07:53:47.051334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_types_counts = df_train.dtypes.value_counts()\nprint(column_types_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:53:49.893537Z","iopub.execute_input":"2024-12-02T07:53:49.89449Z","iopub.status.idle":"2024-12-02T07:53:49.901156Z","shell.execute_reply.started":"2024-12-02T07:53:49.89445Z","shell.execute_reply":"2024-12-02T07:53:49.900007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating aggregated feature like TotalSF and TotalBathrooms can enhance model performance by reducing dimentionality and make pattern easier to learn. also, these features often correlate strongly with house price.","metadata":{}},{"cell_type":"code","source":"#making sure if test data has missing values?\ntest_missing_values=df_test.isnull().sum()\ntest_missing_values=test_missing_values[test_missing_values>0]\nprint(test_missing_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:53:52.669808Z","iopub.execute_input":"2024-12-02T07:53:52.670206Z","iopub.status.idle":"2024-12-02T07:53:52.681837Z","shell.execute_reply.started":"2024-12-02T07:53:52.670173Z","shell.execute_reply":"2024-12-02T07:53:52.680764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So test data has missing value. we should fill them with the amounts we filled train data to prevent data leakage. Also Encode categorical variables using the same encoder objects used for the training data.\n","metadata":{}},{"cell_type":"code","source":"fill_values = {}\nfor col in df_train.columns:\n    if df_train[col].dtype in ['int64', 'float64']:\n        fill_values[col] = df_train[col].median()  # or mean\n    elif df_train[col].dtype == 'object':\n        fill_values[col] = 'missing'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:53:59.531738Z","iopub.execute_input":"2024-12-02T07:53:59.532123Z","iopub.status.idle":"2024-12-02T07:53:59.545994Z","shell.execute_reply.started":"2024-12-02T07:53:59.532091Z","shell.execute_reply":"2024-12-02T07:53:59.544953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Data Cleaning\n#handling missing values\nmissing_values= df_train.isnull().sum()\nmissing_values=missing_values[missing_values>0]\nprint(missing_values)\n\n#filling missing values in train and test\ndf_train = df_train.fillna(fill_values)\ndf_test = df_test.fillna(fill_values)\n\n\nprint(df_train.isnull().sum())\nprint(df_test.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:54:03.007114Z","iopub.execute_input":"2024-12-02T07:54:03.007599Z","iopub.status.idle":"2024-12-02T07:54:03.087789Z","shell.execute_reply.started":"2024-12-02T07:54:03.00756Z","shell.execute_reply":"2024-12-02T07:54:03.086604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Data Preparation\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_combined = pd.concat([df_train, df_test], axis=0)\n\n# Step 3: Convert all categorical columns to string type to avoid mixed data types\nfor col in df_combined.select_dtypes(include='object').columns:\n    df_combined[col] = df_combined[col].astype(str)\n\n# Step 4: Apply label encoding on the combined dataset\nlabel_encoders = {}\nfor col in df_combined.select_dtypes(include='object').columns:\n    le = LabelEncoder()\n    \n    # Fit the encoder on the combined dataset\n    df_combined[col] = le.fit_transform(df_combined[col])\n    \n    # Store the label encoder for later use\n    label_encoders[col] = le\n\n# Step 5: Separate the datasets back\ndf_train_encoded = df_combined[:len(df_train)]\ndf_test_encoded = df_combined[len(df_train):]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:54:07.203777Z","iopub.execute_input":"2024-12-02T07:54:07.204163Z","iopub.status.idle":"2024-12-02T07:54:07.270762Z","shell.execute_reply.started":"2024-12-02T07:54:07.204132Z","shell.execute_reply":"2024-12-02T07:54:07.269616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_encoded=df_test_encoded.drop('SalePrice', axis=1)\nprint(df_test_encoded.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:54:13.626227Z","iopub.execute_input":"2024-12-02T07:54:13.626603Z","iopub.status.idle":"2024-12-02T07:54:13.639574Z","shell.execute_reply.started":"2024-12-02T07:54:13.626572Z","shell.execute_reply":"2024-12-02T07:54:13.63817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#splitting dataset\nX= df_train_encoded.drop('SalePrice', axis=1) \ny = df_train_encoded['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:54:16.499247Z","iopub.execute_input":"2024-12-02T07:54:16.49963Z","iopub.status.idle":"2024-12-02T07:54:16.516186Z","shell.execute_reply.started":"2024-12-02T07:54:16.499598Z","shell.execute_reply":"2024-12-02T07:54:16.515008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tuning hyperparameter\nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodels_and_params = {\n    \"Decision Tree\": {\n        \"model\": DecisionTreeRegressor(),\n        \"params\": {\n            'max_depth': [None, 10, 20, 30],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4]\n        }\n    },\n    \"Random Forest\": {\n        \"model\": RandomForestRegressor(random_state=42),\n        \"params\": {\n            'n_estimators': [100, 200, 300],\n            'max_depth': [None, 10, 20, 30],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4]\n        }\n    },\n    \"Gradient Boosting\": {\n        \"model\": GradientBoostingRegressor(random_state=42),\n        \"params\": {\n            'n_estimators': [100, 200, 300],\n            'learning_rate': [0.01, 0.1, 0.2],\n            'max_depth': [3, 5, 10]\n        }\n    },\n    \"Lasso\": {\n        \"model\": Lasso(),\n        \"params\": {\n            'alpha': [0.01, 0.1, 1, 10, 100]\n        }\n    },\n    \"XGBoost\": {\n        \"model\": XGBRegressor(random_state=42),\n        \"params\": {\n            'n_estimators': [100, 200, 300],\n            'learning_rate': [0.01, 0.1, 0.2],\n            'max_depth': [3, 5, 10],\n            'subsample': [0.8, 1.0],\n            'colsample_bytree': [0.8, 1.0]\n        }\n    }\n}\n\n# Loop through models for hyperparameter tuning\nbest_estimators = {}\nresults = []\n\nfor model_name, model_info in models_and_params.items():\n    print(f\"Tuning {model_name}...\")\n    \n    # Initialize RandomizedSearchCV\n    search = RandomizedSearchCV(\n        estimator=model_info[\"model\"],\n        param_distributions=model_info[\"params\"],\n        scoring='neg_mean_squared_error',\n        cv=5,\n        n_iter=10,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    # Fit the model\n    search.fit(X_train, y_train)\n    \n    # Best estimator and parameters\n    best_estimators[model_name] = search.best_estimator_\n    best_params = search.best_params_\n    \n    # Evaluate on the test set\n    y_pred = search.best_estimator_.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    # Store results\n    results.append({\n        \"Model\": model_name,\n        \"Best Parameters\": best_params,\n        \"MSE\": mse,\n        \"R² Score\": r2\n    })\n\n# Convert results to a DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display results\nprint(results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:54:19.310835Z","iopub.execute_input":"2024-12-02T07:54:19.311914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#finding the best alpha for LASSO\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'alpha': [0.01, 0.1, 1, 10, 100,1000]}  # Define a range of alpha values\ngrid_search = GridSearchCV(\n    Lasso(random_state=42),\n    param_grid,\n    scoring='neg_mean_squared_error',\n    cv=5\n)\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best Alpha: {grid_search.best_params_['alpha']}\")\nprint(f\"Best Cross-Validation MSE: {abs(grid_search.best_score_):.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:46.21236Z","iopub.status.idle":"2024-12-02T07:52:46.212755Z","shell.execute_reply.started":"2024-12-02T07:52:46.212551Z","shell.execute_reply":"2024-12-02T07:52:46.212569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Check model's performance with cross validation\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import make_scorer, mean_squared_error\n\nlasso_model = Lasso(alpha=1000, random_state=42)  # Adjust alpha as needed\nkf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n\n# Use cross_val_score to calculate the scores\nmse_scores = cross_val_score(\n    lasso_model, \n    X_train, \n    y_train, \n    scoring=\"r2\", \n    cv=kf\n)\n\n# Calculate the mean and standard deviation of the MSE\nmean_mse = np.mean(mse_scores)\nstd_mse = np.std(mse_scores)\n\nprint(f\"Cross-Validation Mean r2: {mean_mse:.2f}\")\nprint(f\"Cross-Validation MSE Std Dev: {std_mse:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:46.214987Z","iopub.status.idle":"2024-12-02T07:52:46.215357Z","shell.execute_reply.started":"2024-12-02T07:52:46.215173Z","shell.execute_reply":"2024-12-02T07:52:46.215191Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now training whole train data set with the LASSO and predict test dataset prices.","metadata":{}},{"cell_type":"code","source":"X_train = df_train_encoded.drop('SalePrice', axis=1) \ny_train = df_train_encoded['SalePrice']\nX_test = df_test_encoded\nX_test = X_test[X_train.columns] \n\nlasso = Lasso(alpha=1000)  \nlasso.fit(X_train, y_train)\n\n# Predict on the test dataset\ntest_predictions = lasso.predict(X_test)\n\n# save predictions\npredictions_df = pd.DataFrame({ 'SalesPrice': test_predictions})  \npredictions_df = predictions_df.iloc[1:]\npredictions_df.to_csv('lasso_predictions.csv', index=False)\n\nprint(\"Training on the full dataset is complete. Predictions saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:46.21671Z","iopub.status.idle":"2024-12-02T07:52:46.21707Z","shell.execute_reply.started":"2024-12-02T07:52:46.216897Z","shell.execute_reply":"2024-12-02T07:52:46.216915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# If y was log-transformed:\n# test_preds = np.exp(test_preds)\n\nsubmission = pd.DataFrame({\n    'Id': test_id,\n    'SalePrice': test_predictions\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:52:46.217969Z","iopub.status.idle":"2024-12-02T07:52:46.218328Z","shell.execute_reply.started":"2024-12-02T07:52:46.218148Z","shell.execute_reply":"2024-12-02T07:52:46.218168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}